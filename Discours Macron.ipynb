{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do web requests in python, use \"requests\" library\n",
    "# https://requests.readthedocs.io/en/master/user/quickstart/#make-a-request\n",
    "import requests\n",
    "# To navigate in the dom and get the needed info, use \"BeautifulSoup\" library\n",
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "from bs4 import BeautifulSoup\n",
    "# To create a csv through a datagrame, use \"pandas\" library\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "TIME_MIN_RANDOM_SEC = 2\n",
    "TIME_MAX_RANDOM_SEC = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb page:  77\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "## Goal number 1 : getting all the \"discours\" links of Macron\n",
    "links = []\n",
    "name = \"Macron\"\n",
    "\n",
    "# Preparing the request\n",
    "page = 0\n",
    "maxPage = 0\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36', \n",
    "           'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9'}\n",
    "cookies = {'cookie-agreed': '2'}\n",
    "\n",
    "while page <= maxPage:\n",
    "    url = \"https://www.vie-publique.fr/recherche?search_api_fulltext=\" + name + \"&sort_by=field_update_date&f%5B0%5D=categories%3Adiscours&page=\" + str(page)\n",
    "    # Making the request\n",
    "    r = requests.get(url, headers=headers, cookies=cookies)\n",
    "\n",
    "    # Ckecking the request\n",
    "    if r.status_code != 200:\n",
    "        print('Error webpage status code : ' + r.status_code)\n",
    "        r.raise_for_status()\n",
    "        raise\n",
    "\n",
    "    # Parse the dom\n",
    "    soup = BeautifulSoup(r.text, 'html5lib') \n",
    "    # Getting the number of page from the number of results\n",
    "    if maxPage == 0:\n",
    "        searchCount = soup.find(\"p\", class_=\"count-search\")\n",
    "        nbResultsByPage = 10\n",
    "        nbTotalResults = searchCount.text.split(' ')[0]\n",
    "        maxPage = int(int(nbTotalResults)/nbResultsByPage)\n",
    "        print(\"Nb page: \", maxPage)\n",
    "        \n",
    "        #only for testing\n",
    "        maxPage = 1\n",
    "    \n",
    "    # Getting the links from the current page\n",
    "    searchA = soup.find_all(\"a\", class_=\"link-multiple\")\n",
    "    for tagA in searchA:\n",
    "        links.append(tagA.get('href'))\n",
    "    \n",
    "    page+=1\n",
    "    time.sleep(random.randint(TIME_MIN_RANDOM_SEC,TIME_MAX_RANDOM_SEC))\n",
    "    print(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>speakers</th>\n",
       "      <th>monologue</th>\n",
       "      <th>theme</th>\n",
       "      <th>occasion</th>\n",
       "      <th>tags</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Interview de M. Emmanuel Macron, président de ...</td>\n",
       "      <td>2020-07-14T12:00:00Z</td>\n",
       "      <td>Léa SALAMÉ, France 2\\nBonjour à tous et bonjou...</td>\n",
       "      <td>Emmanuel Macron;Léa Salame</td>\n",
       "      <td>False</td>\n",
       "      <td>Institutions</td>\n",
       "      <td>Fête nationale</td>\n",
       "      <td>Institutions de l'Etat;Politique gouvernementale</td>\n",
       "      <td>https://www.vie-publique.fr/discours/275172-em...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title                  date  \\\n",
       "0  Interview de M. Emmanuel Macron, président de ...  2020-07-14T12:00:00Z   \n",
       "\n",
       "                                                text  \\\n",
       "0  Léa SALAMÉ, France 2\\nBonjour à tous et bonjou...   \n",
       "\n",
       "                     speakers monologue         theme        occasion  \\\n",
       "0  Emmanuel Macron;Léa Salame     False  Institutions  Fête nationale   \n",
       "\n",
       "                                               tags  \\\n",
       "0  Institutions de l'Etat;Politique gouvernementale   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.vie-publique.fr/discours/275172-em...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a new DataFrame\n",
    "# Columns : title: str, date: datetime, text: str, speakers: str (delimiter: ';'), monologue: true/false, \n",
    "# theme: str, occasion: str, tags: str (delimiter: ';', url)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns=['title','date','text','speakers','monologue','theme','occasion','tags','url'])\n",
    "\n",
    "i = 0\n",
    "for url in links:\n",
    "    i+=1\n",
    "    print(i)\n",
    "    # only for testing\n",
    "    if i == 2:\n",
    "        break\n",
    "\n",
    "    r = requests.get(url, headers=headers, cookies=cookies)\n",
    "\n",
    "    # Ckecking the request\n",
    "    if r.status_code != 200:\n",
    "        print('Error webpage status code : ' + r.status_code)\n",
    "        r.raise_for_status()\n",
    "        raise\n",
    "\n",
    "    # Parse the dom\n",
    "    soup = BeautifulSoup(r.text, 'html5lib')\n",
    "\n",
    "    title = soup.h1.text.strip()\n",
    "    \n",
    "    dateSoup = soup.find(\"div\", class_=\"dateBox\")\n",
    "    date = dateSoup.p.span.time.get(\"datetime\") if dateSoup else np.nan\n",
    "    \n",
    "    # get the text, replace <br> with \\n (use text.replace(\"\\n\", \" \") to remove the \"\\n\"), also escape the apostrophe ie. replace ' by \\'\n",
    "    textSoup = soup.find(\"span\", class_=\"field--name-field-texte-integral\")\n",
    "    text = textSoup.p.text if textSoup else np.nan\n",
    "    \"\"\"\n",
    "    # Other method to get all the text\n",
    "    t = soup.find(\"span\", class_=\"field--name-field-texte-integral\").p.stripped_strings\n",
    "    text = [txt for txt in t]\n",
    "    \" \".join(text)\n",
    "    \"\"\"\n",
    "    \n",
    "    speakersSoup = soup.find(\"ul\", class_=\"line-intervenant\")\n",
    "    speakersArray = speakersSoup.find_all(\"li\") if speakersSoup else False\n",
    "    speakers = np.nan\n",
    "    monologue = np.nan\n",
    "    if speakersArray:\n",
    "        speakers = \"\"\n",
    "        for speaker in speakersArray:\n",
    "            speakers += speaker.a.text + \";\"\n",
    "        speakers = speakers[:-1] # Removing the last \";\" (unuseful)\n",
    "        monologue = True if len(speakers.split(\";\")) == 1 else False\n",
    "    \n",
    "    themeSoup = soup.find(\"div\", class_=\"thematicBox\")\n",
    "    theme = np.nan\n",
    "    if themeSoup:\n",
    "        theme = \"\"\n",
    "        for t in themeSoup.find_all(\"li\"):\n",
    "            theme += t.a.text + \";\"\n",
    "        theme = theme[:-1]\n",
    "    \n",
    "    occasionSoup = soup.find(\"span\", class_=\"field--name-field-circonstance\")\n",
    "    occasion = occasionSoup.text if occasionSoup else np.nan\n",
    "    \n",
    "    tagsSoup = soup.find(\"div\", class_=\"tagsBox\")\n",
    "    tags = np.nan\n",
    "    if tagsSoup:\n",
    "        tags = \"\"\n",
    "        for t in tagsSoup.find_all(\"li\"):\n",
    "                tags += t.a.text + \";\"\n",
    "        tags = tags[:-1]\n",
    "    # other method :\n",
    "    # tags = soup.find(\"div\", class_=\"tagsBox\").ul.text.strip().replace(\"  \", \"\").replace(\"\\n\\n\", \"\").replace(\"\\n\",\";\")\n",
    "\n",
    "    # add line\n",
    "    df = df.append({'title': title,'date': date,'text': text,'speakers': speakers,'monologue': monologue,\n",
    "                    'theme': theme,'occasion': occasion,'tags': tags, 'url': url}, ignore_index=True)\n",
    "    \n",
    "    time.sleep(random.randint(TIME_MIN_RANDOM_SEC,TIME_MAX_RANDOM_SEC))\n",
    "\n",
    "df.to_pickle(\"discours_macron.pickle\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"discours_macron.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# May not work\n",
    "df = None\n",
    "df = pd.read_pickle('discours_macron_aws.pickle')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this instead\n",
    "df = pd.read_csv('discours_macron.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['monologue'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "discoursMacronStr = \" \".join(df[df['monologue'] == True]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11477126"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(discoursMacronStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"discours_macron.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(discoursMacronStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('links.pickle','rb') as fp:\n",
    "    links = pickle.load(fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
